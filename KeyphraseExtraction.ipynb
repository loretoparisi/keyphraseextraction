{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": " KeyphraseExtraction.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XaURogq_hP7_",
        "colab_type": "text"
      },
      "source": [
        "**Automatic Keyphrase Extraction**\n",
        "\n",
        "This source code has beed adpted and ported to Python3 from the original Python2 source code available here \n",
        "\n",
        "[Intro to Automatic Keyphrase Extraction](https://bdewilde.github.io/blog/2014/09/23/intro-to-automatic-keyphrase-extraction/)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbvk1U6kg9Nb",
        "colab_type": "code",
        "outputId": "4b70180b-68f5-414e-bb45-355c186f41e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "!pip install nltk\n",
        "!pip install gensim"
      ],
      "execution_count": 184,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from nltk) (1.12.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.6/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.12.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.6/dist-packages (from gensim) (2.0.0)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from gensim) (1.18.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.23.0)\n",
            "Requirement already satisfied: boto3 in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (1.13.13)\n",
            "Requirement already satisfied: boto in /usr/local/lib/python3.6/dist-packages (from smart-open>=1.2.1->gensim) (2.49.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->smart-open>=1.2.1->gensim) (3.0.4)\n",
            "Requirement already satisfied: botocore<1.17.0,>=1.16.13 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (1.16.13)\n",
            "Requirement already satisfied: s3transfer<0.4.0,>=0.3.0 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.3.3)\n",
            "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.6/dist-packages (from boto3->smart-open>=1.2.1->gensim) (0.10.0)\n",
            "Requirement already satisfied: docutils<0.16,>=0.10 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim) (0.15.2)\n",
            "Requirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.6/dist-packages (from botocore<1.17.0,>=1.16.13->boto3->smart-open>=1.2.1->gensim) (2.8.1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jgaUlbhCfq75",
        "colab_type": "text"
      },
      "source": [
        "Place in the next section your paper **ABSTRACT**, **TITLE** and **TEXT**\n",
        "To convert your paper to text use a pdf converter like [PDFElement](https://pdf.wondershare.com/)\n",
        "To copy the text into a string use [this](https://onlinetexttools.com/json-stringify-text) tool."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahCXr44Zfoma",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "title = 'EXPLOITING SYNCHRONIZED LYRICS AND VOCAL FEATURES FOR MUSIC EMOTION DETECTION'\n",
        "abstract = \"One of the key points in music recommendation is authoring engaging playlists according to sentiment and emotions. While previous works were mostly based on audio for music discovery and playlists generation, we take advantage of our synchronized lyrics dataset to combine text representations and music features in a novel way; we therefore introduce the Synchronized Lyrics Emotion Dataset. Unlike other approaches that randomly exploited the audio samples and the whole text, our data is split according to the temporal information provided by the synchronization between lyrics and audio. This work shows a comparison between text-based and audio-based deep learning classiﬁcation models using different techniques from Natural Language Processing and Music Information Retrieval domains. From the experiments on audio we conclude that using vocals only, instead of the whole audio data improves the overall performances of the audio classiﬁer. In the lyrics experiments we exploit the state-ofthe-art word representations applied to the main Deep Learning architectures available in literature. In our benchmarks the results show how the Bilinear LSTM classiﬁer with Attention based on fastText word embedding performs better than the CNN applied on audio. \"\n",
        "text = \"EXPLOITING SYNCHRONIZED LYRICS AND VOCAL FEATURES FOR \\nMUSIC EMOTION DETECTION \\n\\nABSTRACT \\nOne of the key points in music recommendation is authoring engaging playlists according to sentiment and emotions. While previous works were mostly based on audio for music discovery and playlists generation, we take advantage of our synchronized lyrics dataset to combine text representations and music features in a novel way; we therefore introduce the Synchronized Lyrics Emotion Dataset. Unlike other approaches that randomly exploited the audio samples and the whole text, our data is split according to the temporal information provided by the synchronization between lyrics and audio. This work shows a comparison between text-based and audio-based deep learning classiﬁcation models using different techniques from Natural Language Processing and Music Information Retrieval domains. From the experiments on audio we conclude that using vocals only, instead of the whole audio data improves the overall performances of the audio classiﬁer. In the lyrics experiments we exploit the state-ofthe-art word representations applied to the main Deep Learning architectures available in literature. In our benchmarks the results show how the Bilinear LSTM classiﬁer with Attention based on fastText word embedding performs better than the CNN applied on audio. \\n\\n1. INTRODUCTION \\nMusic Emotion Recognition (MER) refers to the task of ﬁnding a relationship between music and human emotions [24,43]. Nowadays, this type of analysis is becoming more and more popular, music streaming providers are ﬁnding very helpful to present users with musical collections organized according to their feelings. The problem of Music Emotion Recognition was proposed for the ﬁrst time in the Music Information Retrieval (MIR) community in 2007, during the annual Music Information Research Evaluation eXchange (MIREX) [14]. Audio and lyrics represent the two main sources from which it is possible to obtain low and high-level features that can accurately describe human moods and emotions perceived while listening to music. An equivalent task can be performed in the area of Natural Language Processing (NLP) analyzing the text information of a song by labeling a sentence, in our case one or more lyrics lines, with the emotion associated to what it expresses. A typical MER approach consists in training \\na classi ﬁer using various representations of the acoustical properties of a musical excerpt such as: timbre, rhythm and harmony [21, 26]. Support Vector Machines are employed with good results also for multilabel classi ﬁcation [30], more recently also Convolutional Neural Networks were used in this ﬁeld [45]. Lyrics-based approaches, on the other hand, make use of Recurrent Neural Networks architectures (like LSTM [13]) for performing text classi ﬁcation [46, 47]. The idea of using lyrics combined with voice only audio signals is done in [29], where emotion recognition is performed by using textual and speech data, instead of visual ones. Measuring and assigning emotions to music is not a straightforward task: the sentiment/mood associated with a song can be derived by a combination of many features, moreover, emotions expressed by a musical excerpt and by its corresponding lyrics do not always match, also, the annotation of mood tags to a song turns out to be highly changing over the song duration [3] and therefore one song can be associated with more than one emotion [45]. There happens to be no uni ﬁed emotion representation in the ﬁeld of MER, meaning that there is no consensus upon which and how many labels are used and if emotions should be considered as categorical or continuous, moreover, emotion annotation has been carried on in different ways over the years, depending on the study and experiments conducted. For this reason, researches have developed many different approaches and results visualization that is hard to track a precise state-of-the-art for this MIR task [45]. In this work, our focus is on describing various approaches for performing emotion classi ﬁcation by analyzing lyrics and audio independently but in a synchronized fashion, as the lyric lines correspond to the portion of audio in which those same lines are sung and the audio is pre-processed using source separation techniques in order to separate the vocal part from the mixture of instruments and retaining the vocal tracks only. \\n\\n2. RELATED WORKS \\nIn this section we provide a description of musical emotion representation and the techniques for performing music emotion classi ﬁcation using audio and lyrics, illustrating various word embedding techniques for text classi ﬁcation. Finally we detail our proposed approach. \\n\\n2.1 Representing musical emotions \\nMany studies were conducted for the representation of musical emotions also in the ﬁeld of psychology. Despite cross-cultural studies suggesting that there may be universal psychophysical and emotional cues that transcend language and acculturation [24], there exist several problems in recognizing moods from music. In fact, one of the main difﬁculties in recognizing musical mood is the ambiguity of human emotions. Different people perceive and feel emotions induced/expressed by music in many distinct ways. Also, their individual way of expressing them using adjectives is biased by a large number of variables and factors, such as the structure of music, the previous experience of the listener, their training, knowledge and psychological condition [12]. Music-IR systems use either categorical descriptions or parametric models of emotion for classi ﬁcation or recognition. Categorical approaches for the representation of emotions comprehend the ﬁnding and the organization of a set of adjectives/tags/labels that are emotional descriptors, based on their relevance and connection with music. One of the ﬁrst studies concerning the aforementioned approach is the one conducted by Hevner and published in 1936, in which the candidates of the experiment were asked to choose from a list of 66 adjectives arranged in 8 groups [12] as shown in Figure 1. Other research, such as the one conducted by Russell [37], suggests that mood can be scaled and measured by a continuum of descriptors. In this scenario, sets of mood descriptors are organized into low-dimensional models, one of those models is the Valence-Arousal (V-A) space (Figure 2), in which emotions are organized on a plane along independent axes of Arousal (intensity, energy) and Valence (pleasantness), ranging from positive to negative. \\n2.2 Music Emotion Classi ﬁcation \\nMER can be approached either as a classi ﬁcation or regression problem in which a musical excerpt or a lyrics sentence is annotated with one or multiple emotion labels or with continuous values such as Valence-Arousal. It starts from the employment of certain acoustic features that resemble timbre, rhythm, harmony and other estimators in order to train Support Vector Machines at classifying the moods [30]. Other techniques examine the use of Gaussian Mixture Models [31, 33] and Naive Bayes classi ﬁers. The features involved for training those classi ﬁers are computed based on the short time Fourier transform (STFT) for each frame of the sound. The most used features are the Mel-frequency cepstral coef ﬁcients (MFCC), spectral features such as the spectral centroid and the spectral ﬂux are important for representing the timbral characteristic of the sound [40]. Neural Networks are employed in [11], where the tempo of the song is computed by means of a multiple agents approach and, starting from other features computed on its statistics, a simple BP neural network classiﬁer is trained to detect the mood. It is not trivial to understand which features are more relevant for the task, therefore feature engineering has been recently carried on with the use of deep architectures (known in this context as fea\\n\\nFigure 1 . Hevner Adjective Clusters. Sixtysix words arranged in eight clusters describing a variety of moods. \\n\\nFigure 2 . Russell ’s Valence-Arousal Space. A twodimensional space representing mood states as continuous numerical values. The combination of valence-arousal values represent different emotions depending on their coordinates on the 2D space.\\n\\nture learning) using either spectral representations of the audio (spectrograms or mel-spectrograms), in [5] feeded to a deep Fully Convolutional Networks (FCN) consisting in convolutional and subsampling layers without any fullyconnected layer or directly using the raw audio signal as input to the network classi ﬁer [9]. \\nEmotion recognition can also be addressed by using a lyrics-based approach. In [7] the probability of various emotions of a song are computed using Latent Dirichlet Allocation (LDA) based on latent semantic topics. In [34] the Russell ’s emotion model is employed and a keyword-based approach is used for classifying each sentence (verse). \\nOther works see the combination of audio-based classiﬁcation with lyrics-based classi ﬁcation in order to improve the informative content of their vector representations and therefore the performances of the classi ﬁcation, as in [1,28,38,44]. In [8] a 100-dimensional word2vec embedding trained on 1.6 million lyrics is tested comparing several architectures (GRU, LSTM, ConvNets). Not only the uni ﬁed feature set results to be an advantage, but also the synchronized combination of both, as suggested in [8]. In [29] speech and text data are used in a fused manner for emotion recognition. \\n2.3 Word Embedding and Text Classi ﬁcation \\nIn the ﬁeld of NLP, text is usually converted into Bag of Words (BoW), Term FrequencyInverse Document Frequency (TF-IDF) and, more recently, highly complex vector representations. In fact in the last few years, Word Embeddings have become an essential part of any DeepLearning-based Natural Language Processing system representing the state-of-the-art for a large variety of classiﬁcation task models. Word Embeddings are pre-trained on a large corpus and can be ﬁne-tuned to automatically improve their performance by incorporating some general representations. The Word2Vec method based on skipgram [35], had a large impact and enabled ef ﬁcient training of dense word representations and a straightforward integration into downstream models. [23, 27, 42] added subword-level information and augmented word embeddings with character information for their relative applications. Later works [2,17] showed that incorporating pretrained embeddings character n-grams features provides more powerful results than composition functions over individual characters for several NLP tasks. Character ngrams are in particular ef ﬁcient and also form the basis of Facebook ’s fastText classi ﬁer [2, 18, 19]. The most recent approaches [15,36] exploit contextual information extracted from bidirectional Deep Neural Models for which a different representation is assigned to each word and it is a function of the part of text to which the word belongs to, gaining state-of-the-art results for most NLP tasks. [16] achieves relevant results and is essentially a method to enable transfer learning for any NLP task without having to train models from scratch. Regarding the prediction models for text classi ﬁcation, LSTM and RNN including all possible model Attention-based variants [47] have repre\\ning, text classi ﬁcation [46] and machine translation [4]; other works show that CNN can be a good approach to solve NLP task too [22]. In the last few years Transformer [41] outperformed both recurrent and convolutional approaches for language understanding, in particular on machine translation and language modeling. \\n2.4 Proposed Approach \\nWe built a new emotion dataset containing synchronized lyrics and audio. Our labels consist in 5 discrete crowdbased adjectives, inspired by the Hevner emotion representation retaining just the basics emotions as in [10]. Our aim is to perform emotion classi ﬁcation using lyrics and audio independently but in a synchronized manner. We have analyzed the performances of different text embedding methods and used contextual feature extraction such as ELMo and BERT combined with various classi ﬁers. We have exploited novel WaveNet [32, 39] techniques for separating singing voice from the audio and used a Convolutional Neural Network for emotion classi ﬁcation. \\n\\n3. SYNCHRONIZED LYRICS EMOTION DATASET \\nPrevious methods combining text and audio for classi ﬁcation purposes were randomly analyzing 30 seconds long segments from the middle of the audio ﬁle [33]. Starting from the idea that the mood can change over time in an audio excerpt, as well as a song might express different moods depending on its lyrics and sentences, we exploit our time-synchronized data and build a novel dataset, the Synchronized Lyrics Emotion Dataset , containing synchronized lyrics collected by the Musixmatch platform having start and duration times in which certain lyrics lines are sung. Musical lyrics are the transcribed words of a song, therefore synchronization is a temporal information that establishes a direct connection between the text and its corresponding audio event interval, in other words, synchronizing lyrics is about storing information on the instance of time (in the audio) at which every lyric line starts and ends. Each sample of the dataset consists of 5 values: the track ID, the lyrics, start/end time information related to the audio ﬁle segments and its relative mood label. The 5 collected emotion labels are shown in Table 1 together with their distribution in the dataset. Each audio segment is a slice of music where the corresponding text is sung and it is tagged with a mood label. Since we need a consistent dimension for our audio features, we chose the audio/text segments to be approximately 30 second long. For simplicity, we distinguish three types of segments: intro, synch, outro . The intro is the portion of a song that goes from the beginning of the song until the starting of the ﬁrst sung line. The synch part goes from the beginning of the ﬁrst sung line until the last one. The outro is a segment that starts from the end of the last sung line until the end of the song. Usually, intro and outro do not contain vocals and, in order to ful ﬁl a consistent analysis, we do not take into account those when analyzing the audio, since our goal is to \\nsented for years the milestone for solving sequence learn\\n\"\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OlkqZHkBhm5T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_candidate_chunks(text, grammar=r'KT: {(<JJ>* <NN.*>+ <IN>)? <JJ>* <NN.*>+}'):\n",
        "    import itertools, nltk, string\n",
        "    \n",
        "    # exclude candidates that are stop words or entirely punctuation\n",
        "    punct = set(string.punctuation)\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    # tokenize, POS-tag, and chunk using regular expressions\n",
        "    chunker = nltk.chunk.regexp.RegexpParser(grammar)\n",
        "    tagged_sents = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))\n",
        "    all_chunks = list(itertools.chain.from_iterable(nltk.chunk.tree2conlltags(chunker.parse(tagged_sent))\n",
        "                                                    for tagged_sent in tagged_sents))\n",
        "    \n",
        "    # join constituent chunk words into a single chunked phrase\n",
        "    candidates = [' '.join(word for word, pos, chunk in group).lower()\n",
        "                  for key, group in itertools.groupby(all_chunks, lambda triple: triple[2] != 'O') if key]\n",
        "\n",
        "    return [cand for cand in candidates\n",
        "            if cand not in stop_words and not all(char in punct for char in cand)]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pO3Z4ayIj3eD",
        "colab_type": "code",
        "outputId": "c8a04a26-accf-480c-b005-fdff4c3c8623",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('averaged_perceptron_tagger')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyXTTchnjzin",
        "colab_type": "code",
        "outputId": "4625c204-9d56-4b4e-bb55-0eee434765d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "set(extract_candidate_chunks(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'100-dimensional word2vec',\n",
              " '] achieves relevant results',\n",
              " '] exploit contextual information',\n",
              " '] speech',\n",
              " '] techniques',\n",
              " 'account',\n",
              " 'acculturation',\n",
              " 'acoustical properties',\n",
              " 'adjectives',\n",
              " 'advantage',\n",
              " 'aforementioned approach',\n",
              " 'aim',\n",
              " 'ambiguity of human emotions',\n",
              " 'annotation of mood tags',\n",
              " 'annual music information research evaluation exchange',\n",
              " 'approach',\n",
              " 'area of natural language processing',\n",
              " 'arousal',\n",
              " 'audio',\n",
              " 'audio classiﬁer',\n",
              " 'audio excerpt',\n",
              " 'audio features',\n",
              " 'audio for classi ﬁcation purposes',\n",
              " 'audio for music discovery',\n",
              " 'audio samples',\n",
              " 'audio segment',\n",
              " 'audio signals',\n",
              " 'audio ﬁle [',\n",
              " 'audio ﬁle segments',\n",
              " 'audio-based deep learning classiﬁcation models',\n",
              " 'audio/text segments',\n",
              " 'bag of words',\n",
              " 'basics emotions',\n",
              " 'basis of facebook ’',\n",
              " 'beginning',\n",
              " 'benchmarks',\n",
              " 'bert',\n",
              " 'bidirectional deep neural models',\n",
              " 'bilinear lstm classiﬁer with attention',\n",
              " 'bow',\n",
              " 'candidates',\n",
              " 'case',\n",
              " 'categorical approaches',\n",
              " 'categorical descriptions',\n",
              " 'certain lyrics lines',\n",
              " 'character ngrams',\n",
              " 'classi ﬁcation',\n",
              " 'classi ﬁer',\n",
              " 'classi ﬁers',\n",
              " 'clusters',\n",
              " 'cnn',\n",
              " 'collected emotion labels',\n",
              " 'combination of audio-based classiﬁcation',\n",
              " 'combination of many features',\n",
              " 'combination of valence-arousal values',\n",
              " 'community',\n",
              " 'comparison',\n",
              " 'complex vector representations',\n",
              " 'connection with music',\n",
              " 'consensus',\n",
              " 'consistent analysis',\n",
              " 'consistent dimension',\n",
              " 'context as fea figure',\n",
              " 'contextual feature extraction',\n",
              " 'continuous values',\n",
              " 'continuum of descriptors',\n",
              " 'convnets',\n",
              " 'convolutional approaches for language understanding',\n",
              " 'convolutional neural network for emotion classi ﬁcation',\n",
              " 'convolutional neural networks',\n",
              " 'coordinates',\n",
              " 'corresponding audio event interval',\n",
              " 'corresponding lyrics',\n",
              " 'corresponding text',\n",
              " 'cross-cultural studies',\n",
              " 'data',\n",
              " 'dataset',\n",
              " 'deep fully convolutional networks',\n",
              " 'deeplearning-based natural language processing system',\n",
              " 'description of musical emotion representation',\n",
              " 'different emotions',\n",
              " 'different moods',\n",
              " 'different people',\n",
              " 'different representation',\n",
              " 'different techniques from natural language processing',\n",
              " 'different ways',\n",
              " 'direct connection',\n",
              " 'discrete',\n",
              " 'distribution',\n",
              " 'duration times',\n",
              " 'ef ﬁcient training of dense word representations',\n",
              " 'elmo',\n",
              " 'embeddings',\n",
              " 'emotion',\n",
              " 'emotion annotation',\n",
              " 'emotion classi ﬁcation',\n",
              " 'emotion model',\n",
              " 'emotion recognition',\n",
              " 'emotional cues',\n",
              " 'emotional descriptors',\n",
              " 'emotions',\n",
              " 'employment of certain acoustic features',\n",
              " 'end',\n",
              " 'ends',\n",
              " 'energy',\n",
              " 'equivalent task',\n",
              " 'essential part',\n",
              " 'experiment',\n",
              " 'experiments',\n",
              " 'experiments on audio',\n",
              " 'exploiting synchronized lyrics and vocal features for music emotion detection abstract',\n",
              " 'fact',\n",
              " 'factors',\n",
              " 'fasttext classi ﬁer [',\n",
              " 'fasttext word',\n",
              " 'fcn',\n",
              " 'feature engineering',\n",
              " 'features',\n",
              " 'feelings',\n",
              " 'figure',\n",
              " 'focus',\n",
              " 'frame',\n",
              " 'fullyconnected layer',\n",
              " 'function',\n",
              " 'fused manner for emotion recognition',\n",
              " 'general representations',\n",
              " 'goal',\n",
              " 'good approach',\n",
              " 'good results',\n",
              " 'groups',\n",
              " 'gru',\n",
              " 'harmony',\n",
              " 'harmony [',\n",
              " 'hevner',\n",
              " 'hevner adjective clusters',\n",
              " 'hevner emotion representation',\n",
              " 'high-level features',\n",
              " 'human emotions',\n",
              " 'human moods',\n",
              " 'idea',\n",
              " 'individual characters for several nlp tasks',\n",
              " 'individual way',\n",
              " 'information',\n",
              " 'informative content',\n",
              " 'ing',\n",
              " 'instance of time',\n",
              " 'intensity',\n",
              " 'intro',\n",
              " 'introduction music emotion recognition',\n",
              " 'key points in music recommendation',\n",
              " 'keyword-based approach',\n",
              " 'knowledge',\n",
              " 'labels',\n",
              " 'language',\n",
              " 'language modeling',\n",
              " 'large corpus',\n",
              " 'large impact',\n",
              " 'large number of variables',\n",
              " 'large variety of classiﬁcation task models',\n",
              " 'last few years',\n",
              " 'last few years transformer',\n",
              " 'last one',\n",
              " 'last sung line',\n",
              " 'latent dirichlet allocation',\n",
              " 'latent semantic topics',\n",
              " 'layers',\n",
              " 'lda',\n",
              " 'list',\n",
              " 'listener',\n",
              " 'literature',\n",
              " 'low-dimensional models',\n",
              " 'lstm',\n",
              " 'lstm [',\n",
              " 'lyric line starts',\n",
              " 'lyric lines correspond',\n",
              " 'lyrics',\n",
              " 'lyrics lines',\n",
              " 'lyrics sentence',\n",
              " 'lyrics-based approach',\n",
              " 'lyrics-based approaches',\n",
              " 'lyrics-based classi ﬁcation in order',\n",
              " 'machine translation',\n",
              " 'main deep learning',\n",
              " 'main difﬁculties',\n",
              " 'main sources',\n",
              " 'many different approaches',\n",
              " 'many labels',\n",
              " 'means',\n",
              " 'mel-frequency cepstral coef ﬁcients',\n",
              " 'mel-spectrograms',\n",
              " 'mer',\n",
              " 'method',\n",
              " 'methods',\n",
              " 'mfcc',\n",
              " 'middle',\n",
              " 'milestone',\n",
              " 'mir',\n",
              " 'mir task',\n",
              " 'mirex',\n",
              " 'mixture of instruments',\n",
              " 'models',\n",
              " 'models from scratch',\n",
              " 'mood',\n",
              " 'mood label',\n",
              " 'mood states as continuous numerical values',\n",
              " 'moods',\n",
              " 'moods from music',\n",
              " 'multilabel classi ﬁcation',\n",
              " 'multiple agents approach',\n",
              " 'multiple emotion labels',\n",
              " 'music',\n",
              " 'music emotion classi ﬁcation',\n",
              " 'music emotion classi ﬁcation mer',\n",
              " 'music features',\n",
              " 'music in many distinct ways',\n",
              " 'music information retrieval',\n",
              " 'music streaming providers',\n",
              " 'music-ir systems',\n",
              " 'musical emotions many studies',\n",
              " 'musical excerpt',\n",
              " 'musical lyrics',\n",
              " 'musical mood',\n",
              " 'musixmatch platform',\n",
              " 'n-grams features',\n",
              " 'naive bayes classi ﬁers',\n",
              " 'network classi ﬁer',\n",
              " 'neural networks',\n",
              " 'new emotion dataset',\n",
              " 'nlp',\n",
              " 'nlp task',\n",
              " 'nlp tasks',\n",
              " 'novel dataset',\n",
              " 'novel wavenet [',\n",
              " 'novel way',\n",
              " 'one',\n",
              " 'order',\n",
              " 'organization',\n",
              " 'other approaches',\n",
              " 'other estimators in order',\n",
              " 'other features',\n",
              " 'other hand',\n",
              " 'other research',\n",
              " 'other techniques',\n",
              " 'other words',\n",
              " 'other works',\n",
              " 'outro',\n",
              " 'overall performances',\n",
              " 'parametric models of emotion',\n",
              " 'part of text',\n",
              " 'particular ef ﬁcient',\n",
              " 'performance',\n",
              " 'performances',\n",
              " 'performances of different text',\n",
              " 'performs',\n",
              " 'plane along independent axes',\n",
              " 'playlists',\n",
              " 'playlists generation',\n",
              " 'pleasantness',\n",
              " 'portion',\n",
              " 'portion of audio',\n",
              " 'possible model attention-based variants',\n",
              " 'powerful results than composition functions',\n",
              " 'precise state-of-the-art',\n",
              " 'prediction models for text classi ﬁcation',\n",
              " 'present users with musical collections',\n",
              " 'previous experience',\n",
              " 'previous works',\n",
              " 'probability of various emotions',\n",
              " 'problem of music emotion recognition',\n",
              " 'psychological condition',\n",
              " 'raw audio signal as input',\n",
              " 'reason',\n",
              " 'recent approaches',\n",
              " 'recognition',\n",
              " 'recurrent',\n",
              " 'refers',\n",
              " 'regression problem',\n",
              " 'related works',\n",
              " 'relationship between music',\n",
              " 'relative applications',\n",
              " 'relative mood label',\n",
              " 'relevance',\n",
              " 'representation of emotions',\n",
              " 'representation of musical emotions',\n",
              " 'researches',\n",
              " 'resemble timbre',\n",
              " 'results',\n",
              " 'results visualization',\n",
              " 'rhythm',\n",
              " 'rnn',\n",
              " 'russell [',\n",
              " 'russell ’',\n",
              " 'same lines',\n",
              " 'sample',\n",
              " 'scenario',\n",
              " 'seconds long segments',\n",
              " 'section',\n",
              " 'segment',\n",
              " 'sentence',\n",
              " 'sentences',\n",
              " 'sentiment',\n",
              " 'sentiment/mood',\n",
              " 'sequence learn',\n",
              " 'set of adjectives/tags/labels',\n",
              " 'sets of mood descriptors',\n",
              " 'several architectures',\n",
              " 'several problems',\n",
              " 'short time fourier transform',\n",
              " 'simple bp neural network classiﬁer',\n",
              " 'simplicity',\n",
              " 'sixtysix words',\n",
              " 'skipgram [',\n",
              " 'slice of music',\n",
              " 'song',\n",
              " 'song duration',\n",
              " 'sound',\n",
              " 'source separation techniques in order',\n",
              " 'space',\n",
              " 'spectral centroid',\n",
              " 'spectral features',\n",
              " 'spectral representations',\n",
              " 'spectral ﬂux',\n",
              " 'speech data',\n",
              " 'start',\n",
              " 'start/end time information',\n",
              " 'starting',\n",
              " 'state-of-the-art',\n",
              " 'state-of-the-art results',\n",
              " 'state-ofthe-art word representations',\n",
              " 'statistics',\n",
              " 'stft',\n",
              " 'straightforward integration into downstream models',\n",
              " 'straightforward task',\n",
              " 'structure of music',\n",
              " 'study',\n",
              " 'subword-level information',\n",
              " 'support vector machines',\n",
              " 'synch',\n",
              " 'synch part',\n",
              " 'synchronization between lyrics',\n",
              " 'synchronized combination',\n",
              " 'synchronized fashion',\n",
              " 'synchronized lyrics',\n",
              " 'synchronized lyrics emotion dataset',\n",
              " 'synchronized lyrics emotion dataset previous methods',\n",
              " 'synchronized manner',\n",
              " 'task',\n",
              " 'techniques',\n",
              " 'techniques for text classi ﬁcation',\n",
              " 'tempo',\n",
              " 'temporal information',\n",
              " 'term frequencyinverse document frequency',\n",
              " 'text',\n",
              " 'text classi ﬁcation',\n",
              " 'text data',\n",
              " 'text information',\n",
              " 'text representations',\n",
              " 'tf-idf',\n",
              " 'therefore synchronization',\n",
              " 'timbral characteristic',\n",
              " 'timbre',\n",
              " 'time',\n",
              " 'time-synchronized data',\n",
              " 'track id',\n",
              " 'training',\n",
              " 'transcribed words',\n",
              " 'transfer',\n",
              " 'ture',\n",
              " 'twodimensional space',\n",
              " 'type of analysis',\n",
              " 'types of segments',\n",
              " 'typical mer approach',\n",
              " 'uni ﬁed emotion representation',\n",
              " 'uni ﬁed feature',\n",
              " 'use of deep architectures',\n",
              " 'use of gaussian mixture models [',\n",
              " 'use of recurrent neural networks',\n",
              " 'used features',\n",
              " 'v-a',\n",
              " 'valence',\n",
              " 'valence-arousal',\n",
              " 'valence-arousal space',\n",
              " 'values',\n",
              " 'variety of moods',\n",
              " 'various approaches',\n",
              " 'various classi ﬁers',\n",
              " 'various representations',\n",
              " 'various word',\n",
              " 'vector representations',\n",
              " 'verse',\n",
              " 'visual ones',\n",
              " 'vocal part',\n",
              " 'vocal tracks',\n",
              " 'vocals',\n",
              " 'voice',\n",
              " 'whole audio data',\n",
              " 'whole text',\n",
              " 'word',\n",
              " 'word embedding',\n",
              " 'word embeddings',\n",
              " 'word embeddings with character information',\n",
              " 'word2vec method',\n",
              " 'work',\n",
              " 'years',\n",
              " 'ﬁeld',\n",
              " 'ﬁeld of mer',\n",
              " 'ﬁeld of nlp',\n",
              " 'ﬁeld of psychology',\n",
              " 'ﬁnding',\n",
              " 'ﬁrst studies',\n",
              " 'ﬁrst sung line',\n",
              " 'ﬁrst time'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 175
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTSRsMgqmanr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_candidate_words(text, good_tags=set(['JJ','JJR','JJS','NN','NNP','NNS','NNPS'])):\n",
        "    import itertools, nltk, string\n",
        "\n",
        "    # exclude candidates that are stop words or entirely punctuation\n",
        "    punct = set(string.punctuation)\n",
        "    stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "    # tokenize and POS-tag words\n",
        "    tagged_words = itertools.chain.from_iterable(nltk.pos_tag_sents(nltk.word_tokenize(sent)\n",
        "                                                                    for sent in nltk.sent_tokenize(text)))\n",
        "    # filter on certain POS tags and lowercase all words\n",
        "    candidates = [word.lower() for word, tag in tagged_words\n",
        "                  if tag in good_tags and word.lower() not in stop_words\n",
        "                  and not all(char in punct for char in word)]\n",
        "\n",
        "    return candidates"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jH1792jp_Yw",
        "colab_type": "code",
        "outputId": "bea418ba-cedb-49d4-8dec-b42dfa595418",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "set(extract_candidate_words(text))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'100-dimensional',\n",
              " 'abstract',\n",
              " 'account',\n",
              " 'acculturation',\n",
              " 'achieves',\n",
              " 'acoustic',\n",
              " 'acoustical',\n",
              " 'adjective',\n",
              " 'adjectives',\n",
              " 'adjectives/tags/labels',\n",
              " 'advantage',\n",
              " 'aforementioned',\n",
              " 'agents',\n",
              " 'aim',\n",
              " 'allocation',\n",
              " 'ambiguity',\n",
              " 'analysis',\n",
              " 'annotation',\n",
              " 'annual',\n",
              " 'applications',\n",
              " 'approach',\n",
              " 'approaches',\n",
              " 'architectures',\n",
              " 'area',\n",
              " 'arousal',\n",
              " 'attention',\n",
              " 'attention-based',\n",
              " 'audio',\n",
              " 'audio-based',\n",
              " 'audio/text',\n",
              " 'available',\n",
              " 'axes',\n",
              " 'bag',\n",
              " 'basics',\n",
              " 'basis',\n",
              " 'bayes',\n",
              " 'beginning',\n",
              " 'benchmarks',\n",
              " 'bert',\n",
              " 'bidirectional',\n",
              " 'bilinear',\n",
              " 'bow',\n",
              " 'bp',\n",
              " 'candidates',\n",
              " 'case',\n",
              " 'categorical',\n",
              " 'centroid',\n",
              " 'cepstral',\n",
              " 'certain',\n",
              " 'character',\n",
              " 'characteristic',\n",
              " 'characters',\n",
              " 'classi',\n",
              " 'classiﬁcation',\n",
              " 'classiﬁer',\n",
              " 'clusters',\n",
              " 'cnn',\n",
              " 'coef',\n",
              " 'collected',\n",
              " 'collections',\n",
              " 'combination',\n",
              " 'community',\n",
              " 'comparison',\n",
              " 'complex',\n",
              " 'composition',\n",
              " 'condition',\n",
              " 'connection',\n",
              " 'consensus',\n",
              " 'consistent',\n",
              " 'content',\n",
              " 'context',\n",
              " 'contextual',\n",
              " 'continuous',\n",
              " 'continuum',\n",
              " 'convnets',\n",
              " 'convolutional',\n",
              " 'coordinates',\n",
              " 'corpus',\n",
              " 'correspond',\n",
              " 'corresponding',\n",
              " 'cross-cultural',\n",
              " 'cues',\n",
              " 'data',\n",
              " 'dataset',\n",
              " 'deep',\n",
              " 'deeplearning-based',\n",
              " 'dense',\n",
              " 'description',\n",
              " 'descriptions',\n",
              " 'descriptors',\n",
              " 'detection',\n",
              " 'different',\n",
              " 'difﬁculties',\n",
              " 'dimension',\n",
              " 'direct',\n",
              " 'dirichlet',\n",
              " 'discovery',\n",
              " 'discrete',\n",
              " 'distinct',\n",
              " 'distribution',\n",
              " 'document',\n",
              " 'downstream',\n",
              " 'duration',\n",
              " 'ef',\n",
              " 'elmo',\n",
              " 'embedding',\n",
              " 'embeddings',\n",
              " 'emotion',\n",
              " 'emotional',\n",
              " 'emotions',\n",
              " 'employment',\n",
              " 'end',\n",
              " 'ends',\n",
              " 'energy',\n",
              " 'engineering',\n",
              " 'equivalent',\n",
              " 'essential',\n",
              " 'estimators',\n",
              " 'evaluation',\n",
              " 'event',\n",
              " 'excerpt',\n",
              " 'exchange',\n",
              " 'experience',\n",
              " 'experiment',\n",
              " 'experiments',\n",
              " 'exploit',\n",
              " 'exploiting',\n",
              " 'extraction',\n",
              " 'facebook',\n",
              " 'fact',\n",
              " 'factors',\n",
              " 'fashion',\n",
              " 'fasttext',\n",
              " 'fcn',\n",
              " 'fea',\n",
              " 'feature',\n",
              " 'features',\n",
              " 'feelings',\n",
              " 'figure',\n",
              " 'focus',\n",
              " 'fourier',\n",
              " 'frame',\n",
              " 'frequency',\n",
              " 'frequencyinverse',\n",
              " 'fully',\n",
              " 'fullyconnected',\n",
              " 'function',\n",
              " 'functions',\n",
              " 'fused',\n",
              " 'gaussian',\n",
              " 'general',\n",
              " 'generation',\n",
              " 'goal',\n",
              " 'good',\n",
              " 'groups',\n",
              " 'gru',\n",
              " 'hand',\n",
              " 'hard',\n",
              " 'harmony',\n",
              " 'helpful',\n",
              " 'hevner',\n",
              " 'high-level',\n",
              " 'human',\n",
              " 'id',\n",
              " 'idea',\n",
              " 'impact',\n",
              " 'important',\n",
              " 'independent',\n",
              " 'individual',\n",
              " 'information',\n",
              " 'informative',\n",
              " 'ing',\n",
              " 'input',\n",
              " 'instance',\n",
              " 'instruments',\n",
              " 'integration',\n",
              " 'intensity',\n",
              " 'interval',\n",
              " 'intro',\n",
              " 'introduction',\n",
              " 'key',\n",
              " 'keyword-based',\n",
              " 'knowledge',\n",
              " 'label',\n",
              " 'labels',\n",
              " 'language',\n",
              " 'large',\n",
              " 'last',\n",
              " 'latent',\n",
              " 'layer',\n",
              " 'layers',\n",
              " 'lda',\n",
              " 'learn',\n",
              " 'learning',\n",
              " 'line',\n",
              " 'lines',\n",
              " 'list',\n",
              " 'listener',\n",
              " 'literature',\n",
              " 'long',\n",
              " 'low',\n",
              " 'low-dimensional',\n",
              " 'lstm',\n",
              " 'lyric',\n",
              " 'lyrics',\n",
              " 'lyrics-based',\n",
              " 'machine',\n",
              " 'machines',\n",
              " 'main',\n",
              " 'manner',\n",
              " 'many',\n",
              " 'means',\n",
              " 'mel-frequency',\n",
              " 'mel-spectrograms',\n",
              " 'mer',\n",
              " 'method',\n",
              " 'methods',\n",
              " 'mfcc',\n",
              " 'middle',\n",
              " 'milestone',\n",
              " 'mir',\n",
              " 'mirex',\n",
              " 'mixture',\n",
              " 'model',\n",
              " 'modeling',\n",
              " 'models',\n",
              " 'mood',\n",
              " 'moods',\n",
              " 'multilabel',\n",
              " 'multiple',\n",
              " 'music',\n",
              " 'music-ir',\n",
              " 'musical',\n",
              " 'musixmatch',\n",
              " 'n-grams',\n",
              " 'naive',\n",
              " 'natural',\n",
              " 'negative',\n",
              " 'network',\n",
              " 'networks',\n",
              " 'neural',\n",
              " 'new',\n",
              " 'ngrams',\n",
              " 'nlp',\n",
              " 'novel',\n",
              " 'number',\n",
              " 'numerical',\n",
              " 'one',\n",
              " 'ones',\n",
              " 'order',\n",
              " 'organization',\n",
              " 'outro',\n",
              " 'overall',\n",
              " 'parametric',\n",
              " 'part',\n",
              " 'particular',\n",
              " 'people',\n",
              " 'performance',\n",
              " 'performances',\n",
              " 'performs',\n",
              " 'plane',\n",
              " 'platform',\n",
              " 'playlists',\n",
              " 'pleasantness',\n",
              " 'points',\n",
              " 'popular',\n",
              " 'portion',\n",
              " 'positive',\n",
              " 'possible',\n",
              " 'powerful',\n",
              " 'pre-processed',\n",
              " 'pre-trained',\n",
              " 'precise',\n",
              " 'prediction',\n",
              " 'present',\n",
              " 'previous',\n",
              " 'probability',\n",
              " 'problem',\n",
              " 'problems',\n",
              " 'processing',\n",
              " 'properties',\n",
              " 'providers',\n",
              " 'psychological',\n",
              " 'psychology',\n",
              " 'psychophysical',\n",
              " 'purposes',\n",
              " 'raw',\n",
              " 'reason',\n",
              " 'recent',\n",
              " 'recognition',\n",
              " 'recommendation',\n",
              " 'recurrent',\n",
              " 'refers',\n",
              " 'regression',\n",
              " 'related',\n",
              " 'relationship',\n",
              " 'relative',\n",
              " 'relevance',\n",
              " 'relevant',\n",
              " 'representation',\n",
              " 'representations',\n",
              " 'research',\n",
              " 'researches',\n",
              " 'resemble',\n",
              " 'results',\n",
              " 'retrieval',\n",
              " 'rhythm',\n",
              " 'rnn',\n",
              " 'russell',\n",
              " 'sample',\n",
              " 'samples',\n",
              " 'scenario',\n",
              " 'scratch',\n",
              " 'second',\n",
              " 'seconds',\n",
              " 'section',\n",
              " 'segment',\n",
              " 'segments',\n",
              " 'semantic',\n",
              " 'sentence',\n",
              " 'sentences',\n",
              " 'sentiment',\n",
              " 'sentiment/mood',\n",
              " 'separation',\n",
              " 'sequence',\n",
              " 'set',\n",
              " 'sets',\n",
              " 'several',\n",
              " 'short',\n",
              " 'signal',\n",
              " 'signals',\n",
              " 'simple',\n",
              " 'simplicity',\n",
              " 'sixtysix',\n",
              " 'skipgram',\n",
              " 'slice',\n",
              " 'song',\n",
              " 'sound',\n",
              " 'source',\n",
              " 'sources',\n",
              " 'space',\n",
              " 'spectral',\n",
              " 'spectrograms',\n",
              " 'speech',\n",
              " 'split',\n",
              " 'start',\n",
              " 'start/end',\n",
              " 'starting',\n",
              " 'starts',\n",
              " 'state-of-the-art',\n",
              " 'state-ofthe-art',\n",
              " 'states',\n",
              " 'statistics',\n",
              " 'stft',\n",
              " 'straightforward',\n",
              " 'streaming',\n",
              " 'structure',\n",
              " 'studies',\n",
              " 'study',\n",
              " 'subword-level',\n",
              " 'sung',\n",
              " 'support',\n",
              " 'synch',\n",
              " 'synchronization',\n",
              " 'synchronized',\n",
              " 'system',\n",
              " 'systems',\n",
              " 'table',\n",
              " 'tags',\n",
              " 'task',\n",
              " 'tasks',\n",
              " 'techniques',\n",
              " 'tempo',\n",
              " 'temporal',\n",
              " 'term',\n",
              " 'text',\n",
              " 'text-based',\n",
              " 'textual',\n",
              " 'tf-idf',\n",
              " 'therefore',\n",
              " 'timbral',\n",
              " 'timbre',\n",
              " 'time',\n",
              " 'time-synchronized',\n",
              " 'times',\n",
              " 'topics',\n",
              " 'track',\n",
              " 'tracks',\n",
              " 'training',\n",
              " 'transcribed',\n",
              " 'transfer',\n",
              " 'transform',\n",
              " 'transformer',\n",
              " 'translation',\n",
              " 'trivial',\n",
              " 'ture',\n",
              " 'twodimensional',\n",
              " 'type',\n",
              " 'types',\n",
              " 'typical',\n",
              " 'understanding',\n",
              " 'uni',\n",
              " 'universal',\n",
              " 'use',\n",
              " 'used',\n",
              " 'users',\n",
              " 'v-a',\n",
              " 'valence',\n",
              " 'valence-arousal',\n",
              " 'values',\n",
              " 'variables',\n",
              " 'variants',\n",
              " 'variety',\n",
              " 'various',\n",
              " 'vector',\n",
              " 'verse',\n",
              " 'visual',\n",
              " 'visualization',\n",
              " 'vocal',\n",
              " 'vocals',\n",
              " 'voice',\n",
              " 'wavenet',\n",
              " 'way',\n",
              " 'ways',\n",
              " 'whole',\n",
              " 'word',\n",
              " 'word2vec',\n",
              " 'words',\n",
              " 'work',\n",
              " 'works',\n",
              " 'years',\n",
              " '’',\n",
              " 'ﬁcation',\n",
              " 'ﬁcient',\n",
              " 'ﬁcients',\n",
              " 'ﬁed',\n",
              " 'ﬁeld',\n",
              " 'ﬁer',\n",
              " 'ﬁers',\n",
              " 'ﬁle',\n",
              " 'ﬁnding',\n",
              " 'ﬁne-tuned',\n",
              " 'ﬁrst',\n",
              " 'ﬂux'}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 176
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u63SWPwgmecy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_keyphrases_by_tfidf(texts, candidates='chunks'):\n",
        "    import gensim, nltk\n",
        "    \n",
        "    # extract candidates from each text in texts, either chunks or words\n",
        "    if candidates == 'chunks':\n",
        "        boc_texts = [extract_candidate_chunks(text) for text in texts]\n",
        "    elif candidates == 'words':\n",
        "        boc_texts = [extract_candidate_words(text) for text in texts]\n",
        "    # make gensim dictionary and corpus\n",
        "    dictionary = gensim.corpora.Dictionary(boc_texts)\n",
        "    corpus = [dictionary.doc2bow(boc_text) for boc_text in boc_texts]\n",
        "    # transform corpus with tf*idf model\n",
        "    tfidf = gensim.models.TfidfModel(corpus)\n",
        "    corpus_tfidf = tfidf[corpus]\n",
        "    return corpus, corpus_tfidf, dictionary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jp9QEzqjqdML",
        "colab_type": "code",
        "outputId": "47ec410e-2428-43a4-cccb-9152bd28b001",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "texts = [abstract, text]\n",
        "corpus, corpus_tfidf, dictionary = score_keyphrases_by_tfidf(texts, candidates='chunks')\n",
        "#for k, v in dictionary.token2id.items():\n",
        "#    print(f'{k:{15}} {v:{10}}')\n",
        "print(dictionary.token2id)\n",
        "word_frequencies = [[(dictionary[id], frequence) for id, frequence in couple] for couple in corpus]\n",
        "print(word_frequencies)\n",
        "# d = {dictionary.get(id): value for doc in corpus_tfidf for id, value in doc}\n",
        "d = {}\n",
        "for doc in corpus_tfidf:\n",
        "    for id, value in doc:\n",
        "        word = dictionary.get(id)\n",
        "        d[word] = value\n",
        "#print(d)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'advantage': 0, 'audio': 1, 'audio classiﬁer': 2, 'audio for music discovery': 3, 'audio samples': 4, 'audio-based deep learning classiﬁcation models': 5, 'benchmarks': 6, 'bilinear lstm classiﬁer with attention': 7, 'cnn': 8, 'comparison': 9, 'data': 10, 'different techniques from natural language processing': 11, 'emotions': 12, 'experiments on audio': 13, 'fasttext word': 14, 'key points in music recommendation': 15, 'literature': 16, 'lyrics': 17, 'main deep learning': 18, 'music features': 19, 'music information retrieval': 20, 'novel way': 21, 'other approaches': 22, 'overall performances': 23, 'performs': 24, 'playlists': 25, 'playlists generation': 26, 'previous works': 27, 'results': 28, 'sentiment': 29, 'state-ofthe-art word representations': 30, 'synchronization between lyrics': 31, 'synchronized lyrics': 32, 'synchronized lyrics emotion dataset': 33, 'temporal information': 34, 'text representations': 35, 'vocals': 36, 'whole audio data': 37, 'whole text': 38, 'work': 39, '100-dimensional word2vec': 40, '] achieves relevant results': 41, '] exploit contextual information': 42, '] speech': 43, '] techniques': 44, 'account': 45, 'acculturation': 46, 'acoustical properties': 47, 'adjectives': 48, 'aforementioned approach': 49, 'aim': 50, 'ambiguity of human emotions': 51, 'annotation of mood tags': 52, 'annual music information research evaluation exchange': 53, 'approach': 54, 'area of natural language processing': 55, 'arousal': 56, 'audio excerpt': 57, 'audio features': 58, 'audio for classi ﬁcation purposes': 59, 'audio segment': 60, 'audio signals': 61, 'audio ﬁle [': 62, 'audio ﬁle segments': 63, 'audio/text segments': 64, 'bag of words': 65, 'basics emotions': 66, 'basis of facebook ’': 67, 'beginning': 68, 'bert': 69, 'bidirectional deep neural models': 70, 'bow': 71, 'candidates': 72, 'case': 73, 'categorical approaches': 74, 'categorical descriptions': 75, 'certain lyrics lines': 76, 'character ngrams': 77, 'classi ﬁcation': 78, 'classi ﬁer': 79, 'classi ﬁers': 80, 'clusters': 81, 'collected emotion labels': 82, 'combination of audio-based classiﬁcation': 83, 'combination of many features': 84, 'combination of valence-arousal values': 85, 'community': 86, 'complex vector representations': 87, 'connection with music': 88, 'consensus': 89, 'consistent analysis': 90, 'consistent dimension': 91, 'context as fea figure': 92, 'contextual feature extraction': 93, 'continuous values': 94, 'continuum of descriptors': 95, 'convnets': 96, 'convolutional approaches for language understanding': 97, 'convolutional neural network for emotion classi ﬁcation': 98, 'convolutional neural networks': 99, 'coordinates': 100, 'corresponding audio event interval': 101, 'corresponding lyrics': 102, 'corresponding text': 103, 'cross-cultural studies': 104, 'dataset': 105, 'deep fully convolutional networks': 106, 'deeplearning-based natural language processing system': 107, 'description of musical emotion representation': 108, 'different emotions': 109, 'different moods': 110, 'different people': 111, 'different representation': 112, 'different ways': 113, 'direct connection': 114, 'discrete': 115, 'distribution': 116, 'duration times': 117, 'ef ﬁcient training of dense word representations': 118, 'elmo': 119, 'embeddings': 120, 'emotion': 121, 'emotion annotation': 122, 'emotion classi ﬁcation': 123, 'emotion model': 124, 'emotion recognition': 125, 'emotional cues': 126, 'emotional descriptors': 127, 'employment of certain acoustic features': 128, 'end': 129, 'ends': 130, 'energy': 131, 'equivalent task': 132, 'essential part': 133, 'experiment': 134, 'experiments': 135, 'exploiting synchronized lyrics and vocal features for music emotion detection abstract': 136, 'fact': 137, 'factors': 138, 'fasttext classi ﬁer [': 139, 'fcn': 140, 'feature engineering': 141, 'features': 142, 'feelings': 143, 'figure': 144, 'focus': 145, 'frame': 146, 'fullyconnected layer': 147, 'function': 148, 'fused manner for emotion recognition': 149, 'general representations': 150, 'goal': 151, 'good approach': 152, 'good results': 153, 'groups': 154, 'gru': 155, 'harmony': 156, 'harmony [': 157, 'hevner': 158, 'hevner adjective clusters': 159, 'hevner emotion representation': 160, 'high-level features': 161, 'human emotions': 162, 'human moods': 163, 'idea': 164, 'individual characters for several nlp tasks': 165, 'individual way': 166, 'information': 167, 'informative content': 168, 'ing': 169, 'instance of time': 170, 'intensity': 171, 'intro': 172, 'introduction music emotion recognition': 173, 'keyword-based approach': 174, 'knowledge': 175, 'labels': 176, 'language': 177, 'language modeling': 178, 'large corpus': 179, 'large impact': 180, 'large number of variables': 181, 'large variety of classiﬁcation task models': 182, 'last few years': 183, 'last few years transformer': 184, 'last one': 185, 'last sung line': 186, 'latent dirichlet allocation': 187, 'latent semantic topics': 188, 'layers': 189, 'lda': 190, 'list': 191, 'listener': 192, 'low-dimensional models': 193, 'lstm': 194, 'lstm [': 195, 'lyric line starts': 196, 'lyric lines correspond': 197, 'lyrics lines': 198, 'lyrics sentence': 199, 'lyrics-based approach': 200, 'lyrics-based approaches': 201, 'lyrics-based classi ﬁcation in order': 202, 'machine translation': 203, 'main difﬁculties': 204, 'main sources': 205, 'many different approaches': 206, 'many labels': 207, 'means': 208, 'mel-frequency cepstral coef ﬁcients': 209, 'mel-spectrograms': 210, 'mer': 211, 'method': 212, 'methods': 213, 'mfcc': 214, 'middle': 215, 'milestone': 216, 'mir': 217, 'mir task': 218, 'mirex': 219, 'mixture of instruments': 220, 'models': 221, 'models from scratch': 222, 'mood': 223, 'mood label': 224, 'mood states as continuous numerical values': 225, 'moods': 226, 'moods from music': 227, 'multilabel classi ﬁcation': 228, 'multiple agents approach': 229, 'multiple emotion labels': 230, 'music': 231, 'music emotion classi ﬁcation': 232, 'music emotion classi ﬁcation mer': 233, 'music in many distinct ways': 234, 'music streaming providers': 235, 'music-ir systems': 236, 'musical emotions many studies': 237, 'musical excerpt': 238, 'musical lyrics': 239, 'musical mood': 240, 'musixmatch platform': 241, 'n-grams features': 242, 'naive bayes classi ﬁers': 243, 'network classi ﬁer': 244, 'neural networks': 245, 'new emotion dataset': 246, 'nlp': 247, 'nlp task': 248, 'nlp tasks': 249, 'novel dataset': 250, 'novel wavenet [': 251, 'one': 252, 'order': 253, 'organization': 254, 'other estimators in order': 255, 'other features': 256, 'other hand': 257, 'other research': 258, 'other techniques': 259, 'other words': 260, 'other works': 261, 'outro': 262, 'parametric models of emotion': 263, 'part of text': 264, 'particular ef ﬁcient': 265, 'performance': 266, 'performances': 267, 'performances of different text': 268, 'plane along independent axes': 269, 'pleasantness': 270, 'portion': 271, 'portion of audio': 272, 'possible model attention-based variants': 273, 'powerful results than composition functions': 274, 'precise state-of-the-art': 275, 'prediction models for text classi ﬁcation': 276, 'present users with musical collections': 277, 'previous experience': 278, 'probability of various emotions': 279, 'problem of music emotion recognition': 280, 'psychological condition': 281, 'raw audio signal as input': 282, 'reason': 283, 'recent approaches': 284, 'recognition': 285, 'recurrent': 286, 'refers': 287, 'regression problem': 288, 'related works': 289, 'relationship between music': 290, 'relative applications': 291, 'relative mood label': 292, 'relevance': 293, 'representation of emotions': 294, 'representation of musical emotions': 295, 'researches': 296, 'resemble timbre': 297, 'results visualization': 298, 'rhythm': 299, 'rnn': 300, 'russell [': 301, 'russell ’': 302, 'same lines': 303, 'sample': 304, 'scenario': 305, 'seconds long segments': 306, 'section': 307, 'segment': 308, 'sentence': 309, 'sentences': 310, 'sentiment/mood': 311, 'sequence learn': 312, 'set of adjectives/tags/labels': 313, 'sets of mood descriptors': 314, 'several architectures': 315, 'several problems': 316, 'short time fourier transform': 317, 'simple bp neural network classiﬁer': 318, 'simplicity': 319, 'sixtysix words': 320, 'skipgram [': 321, 'slice of music': 322, 'song': 323, 'song duration': 324, 'sound': 325, 'source separation techniques in order': 326, 'space': 327, 'spectral centroid': 328, 'spectral features': 329, 'spectral representations': 330, 'spectral ﬂux': 331, 'speech data': 332, 'start': 333, 'start/end time information': 334, 'starting': 335, 'state-of-the-art': 336, 'state-of-the-art results': 337, 'statistics': 338, 'stft': 339, 'straightforward integration into downstream models': 340, 'straightforward task': 341, 'structure of music': 342, 'study': 343, 'subword-level information': 344, 'support vector machines': 345, 'synch': 346, 'synch part': 347, 'synchronized combination': 348, 'synchronized fashion': 349, 'synchronized lyrics emotion dataset previous methods': 350, 'synchronized manner': 351, 'task': 352, 'techniques': 353, 'techniques for text classi ﬁcation': 354, 'tempo': 355, 'term frequencyinverse document frequency': 356, 'text': 357, 'text classi ﬁcation': 358, 'text data': 359, 'text information': 360, 'tf-idf': 361, 'therefore synchronization': 362, 'timbral characteristic': 363, 'timbre': 364, 'time': 365, 'time-synchronized data': 366, 'track id': 367, 'training': 368, 'transcribed words': 369, 'transfer': 370, 'ture': 371, 'twodimensional space': 372, 'type of analysis': 373, 'types of segments': 374, 'typical mer approach': 375, 'uni ﬁed emotion representation': 376, 'uni ﬁed feature': 377, 'use of deep architectures': 378, 'use of gaussian mixture models [': 379, 'use of recurrent neural networks': 380, 'used features': 381, 'v-a': 382, 'valence': 383, 'valence-arousal': 384, 'valence-arousal space': 385, 'values': 386, 'variety of moods': 387, 'various approaches': 388, 'various classi ﬁers': 389, 'various representations': 390, 'various word': 391, 'vector representations': 392, 'verse': 393, 'visual ones': 394, 'vocal part': 395, 'vocal tracks': 396, 'voice': 397, 'word': 398, 'word embedding': 399, 'word embeddings': 400, 'word embeddings with character information': 401, 'word2vec method': 402, 'years': 403, 'ﬁeld': 404, 'ﬁeld of mer': 405, 'ﬁeld of nlp': 406, 'ﬁeld of psychology': 407, 'ﬁnding': 408, 'ﬁrst studies': 409, 'ﬁrst sung line': 410, 'ﬁrst time': 411}\n",
            "[[('advantage', 1), ('audio', 2), ('audio classiﬁer', 1), ('audio for music discovery', 1), ('audio samples', 1), ('audio-based deep learning classiﬁcation models', 1), ('benchmarks', 1), ('bilinear lstm classiﬁer with attention', 1), ('cnn', 1), ('comparison', 1), ('data', 1), ('different techniques from natural language processing', 1), ('emotions', 1), ('experiments on audio', 1), ('fasttext word', 1), ('key points in music recommendation', 1), ('literature', 1), ('lyrics', 1), ('main deep learning', 1), ('music features', 1), ('music information retrieval', 1), ('novel way', 1), ('other approaches', 1), ('overall performances', 1), ('performs', 1), ('playlists', 1), ('playlists generation', 1), ('previous works', 1), ('results', 1), ('sentiment', 1), ('state-ofthe-art word representations', 1), ('synchronization between lyrics', 1), ('synchronized lyrics', 1), ('synchronized lyrics emotion dataset', 1), ('temporal information', 1), ('text representations', 1), ('vocals', 1), ('whole audio data', 1), ('whole text', 1), ('work', 1)], [('advantage', 2), ('audio', 9), ('audio classiﬁer', 1), ('audio for music discovery', 1), ('audio samples', 1), ('audio-based deep learning classiﬁcation models', 1), ('benchmarks', 1), ('bilinear lstm classiﬁer with attention', 1), ('cnn', 2), ('comparison', 1), ('data', 1), ('different techniques from natural language processing', 1), ('emotions', 7), ('experiments on audio', 1), ('fasttext word', 1), ('key points in music recommendation', 1), ('literature', 1), ('lyrics', 12), ('main deep learning', 1), ('music features', 1), ('music information retrieval', 2), ('novel way', 1), ('other approaches', 1), ('overall performances', 1), ('performs', 1), ('playlists', 1), ('playlists generation', 1), ('previous works', 1), ('results', 2), ('sentiment', 1), ('state-ofthe-art word representations', 1), ('synchronization between lyrics', 1), ('synchronized lyrics', 1), ('synchronized lyrics emotion dataset', 2), ('temporal information', 2), ('text representations', 1), ('vocals', 2), ('whole audio data', 1), ('whole text', 1), ('work', 2), ('100-dimensional word2vec', 1), ('] achieves relevant results', 1), ('] exploit contextual information', 1), ('] speech', 1), ('] techniques', 1), ('account', 1), ('acculturation', 1), ('acoustical properties', 1), ('adjectives', 3), ('aforementioned approach', 1), ('aim', 1), ('ambiguity of human emotions', 1), ('annotation of mood tags', 1), ('annual music information research evaluation exchange', 1), ('approach', 2), ('area of natural language processing', 1), ('arousal', 1), ('audio excerpt', 1), ('audio features', 1), ('audio for classi ﬁcation purposes', 1), ('audio segment', 1), ('audio signals', 1), ('audio ﬁle [', 1), ('audio ﬁle segments', 1), ('audio/text segments', 1), ('bag of words', 1), ('basics emotions', 1), ('basis of facebook ’', 1), ('beginning', 2), ('bert', 1), ('bidirectional deep neural models', 1), ('bow', 1), ('candidates', 1), ('case', 1), ('categorical approaches', 1), ('categorical descriptions', 1), ('certain lyrics lines', 1), ('character ngrams', 1), ('classi ﬁcation', 3), ('classi ﬁer', 1), ('classi ﬁers', 1), ('clusters', 1), ('collected emotion labels', 1), ('combination of audio-based classiﬁcation', 1), ('combination of many features', 1), ('combination of valence-arousal values', 1), ('community', 1), ('complex vector representations', 1), ('connection with music', 1), ('consensus', 1), ('consistent analysis', 1), ('consistent dimension', 1), ('context as fea figure', 1), ('contextual feature extraction', 1), ('continuous values', 1), ('continuum of descriptors', 1), ('convnets', 1), ('convolutional approaches for language understanding', 1), ('convolutional neural network for emotion classi ﬁcation', 1), ('convolutional neural networks', 1), ('coordinates', 1), ('corresponding audio event interval', 1), ('corresponding lyrics', 1), ('corresponding text', 1), ('cross-cultural studies', 1), ('dataset', 2), ('deep fully convolutional networks', 1), ('deeplearning-based natural language processing system', 1), ('description of musical emotion representation', 1), ('different emotions', 1), ('different moods', 1), ('different people', 1), ('different representation', 1), ('different ways', 1), ('direct connection', 1), ('discrete', 1), ('distribution', 1), ('duration times', 1), ('ef ﬁcient training of dense word representations', 1), ('elmo', 1), ('embeddings', 1), ('emotion', 2), ('emotion annotation', 1), ('emotion classi ﬁcation', 2), ('emotion model', 1), ('emotion recognition', 2), ('emotional cues', 1), ('emotional descriptors', 1), ('employment of certain acoustic features', 1), ('end', 2), ('ends', 1), ('energy', 1), ('equivalent task', 1), ('essential part', 1), ('experiment', 1), ('experiments', 1), ('exploiting synchronized lyrics and vocal features for music emotion detection abstract', 1), ('fact', 2), ('factors', 1), ('fasttext classi ﬁer [', 1), ('fcn', 1), ('feature engineering', 1), ('features', 2), ('feelings', 1), ('figure', 3), ('focus', 1), ('frame', 1), ('fullyconnected layer', 1), ('function', 1), ('fused manner for emotion recognition', 1), ('general representations', 1), ('goal', 1), ('good approach', 1), ('good results', 1), ('groups', 1), ('gru', 1), ('harmony', 1), ('harmony [', 1), ('hevner', 1), ('hevner adjective clusters', 1), ('hevner emotion representation', 1), ('high-level features', 1), ('human emotions', 1), ('human moods', 1), ('idea', 2), ('individual characters for several nlp tasks', 1), ('individual way', 1), ('information', 1), ('informative content', 1), ('ing', 1), ('instance of time', 1), ('intensity', 1), ('intro', 3), ('introduction music emotion recognition', 1), ('keyword-based approach', 1), ('knowledge', 1), ('labels', 1), ('language', 1), ('language modeling', 1), ('large corpus', 1), ('large impact', 1), ('large number of variables', 1), ('large variety of classiﬁcation task models', 1), ('last few years', 1), ('last few years transformer', 1), ('last one', 1), ('last sung line', 1), ('latent dirichlet allocation', 1), ('latent semantic topics', 1), ('layers', 1), ('lda', 1), ('list', 1), ('listener', 1), ('low-dimensional models', 1), ('lstm', 2), ('lstm [', 1), ('lyric line starts', 1), ('lyric lines correspond', 1), ('lyrics lines', 1), ('lyrics sentence', 1), ('lyrics-based approach', 1), ('lyrics-based approaches', 1), ('lyrics-based classi ﬁcation in order', 1), ('machine translation', 2), ('main difﬁculties', 1), ('main sources', 1), ('many different approaches', 1), ('many labels', 1), ('means', 1), ('mel-frequency cepstral coef ﬁcients', 1), ('mel-spectrograms', 1), ('mer', 1), ('method', 1), ('methods', 1), ('mfcc', 1), ('middle', 1), ('milestone', 1), ('mir', 1), ('mir task', 1), ('mirex', 1), ('mixture of instruments', 1), ('models', 1), ('models from scratch', 1), ('mood', 3), ('mood label', 1), ('mood states as continuous numerical values', 1), ('moods', 1), ('moods from music', 1), ('multilabel classi ﬁcation', 1), ('multiple agents approach', 1), ('multiple emotion labels', 1), ('music', 2), ('music emotion classi ﬁcation', 1), ('music emotion classi ﬁcation mer', 1), ('music in many distinct ways', 1), ('music streaming providers', 1), ('music-ir systems', 1), ('musical emotions many studies', 1), ('musical excerpt', 3), ('musical lyrics', 1), ('musical mood', 1), ('musixmatch platform', 1), ('n-grams features', 1), ('naive bayes classi ﬁers', 1), ('network classi ﬁer', 1), ('neural networks', 1), ('new emotion dataset', 1), ('nlp', 1), ('nlp task', 2), ('nlp tasks', 1), ('novel dataset', 1), ('novel wavenet [', 1), ('one', 1), ('order', 1), ('organization', 1), ('other estimators in order', 1), ('other features', 1), ('other hand', 1), ('other research', 1), ('other techniques', 1), ('other words', 1), ('other works', 1), ('outro', 2), ('parametric models of emotion', 1), ('part of text', 1), ('particular ef ﬁcient', 1), ('performance', 1), ('performances', 1), ('performances of different text', 1), ('plane along independent axes', 1), ('pleasantness', 1), ('portion', 1), ('portion of audio', 1), ('possible model attention-based variants', 1), ('powerful results than composition functions', 1), ('precise state-of-the-art', 1), ('prediction models for text classi ﬁcation', 1), ('present users with musical collections', 1), ('previous experience', 1), ('probability of various emotions', 1), ('problem of music emotion recognition', 1), ('psychological condition', 1), ('raw audio signal as input', 1), ('reason', 1), ('recent approaches', 1), ('recognition', 1), ('recurrent', 1), ('refers', 1), ('regression problem', 1), ('related works', 1), ('relationship between music', 1), ('relative applications', 1), ('relative mood label', 1), ('relevance', 1), ('representation of emotions', 1), ('representation of musical emotions', 1), ('researches', 1), ('resemble timbre', 1), ('results visualization', 1), ('rhythm', 2), ('rnn', 1), ('russell [', 1), ('russell ’', 2), ('same lines', 1), ('sample', 1), ('scenario', 1), ('seconds long segments', 1), ('section', 1), ('segment', 1), ('sentence', 2), ('sentences', 1), ('sentiment/mood', 1), ('sequence learn', 1), ('set of adjectives/tags/labels', 1), ('sets of mood descriptors', 1), ('several architectures', 1), ('several problems', 1), ('short time fourier transform', 1), ('simple bp neural network classiﬁer', 1), ('simplicity', 1), ('sixtysix words', 1), ('skipgram [', 1), ('slice of music', 1), ('song', 11), ('song duration', 1), ('sound', 2), ('source separation techniques in order', 1), ('space', 2), ('spectral centroid', 1), ('spectral features', 1), ('spectral representations', 1), ('spectral ﬂux', 1), ('speech data', 1), ('start', 1), ('start/end time information', 1), ('starting', 1), ('state-of-the-art', 1), ('state-of-the-art results', 1), ('statistics', 1), ('stft', 1), ('straightforward integration into downstream models', 1), ('straightforward task', 1), ('structure of music', 1), ('study', 1), ('subword-level information', 1), ('support vector machines', 2), ('synch', 1), ('synch part', 1), ('synchronized combination', 1), ('synchronized fashion', 1), ('synchronized lyrics emotion dataset previous methods', 1), ('synchronized manner', 1), ('task', 2), ('techniques', 1), ('techniques for text classi ﬁcation', 1), ('tempo', 1), ('term frequencyinverse document frequency', 1), ('text', 3), ('text classi ﬁcation', 3), ('text data', 1), ('text information', 1), ('tf-idf', 1), ('therefore synchronization', 1), ('timbral characteristic', 1), ('timbre', 1), ('time', 1), ('time-synchronized data', 1), ('track id', 1), ('training', 1), ('transcribed words', 1), ('transfer', 1), ('ture', 1), ('twodimensional space', 1), ('type of analysis', 1), ('types of segments', 1), ('typical mer approach', 1), ('uni ﬁed emotion representation', 1), ('uni ﬁed feature', 1), ('use of deep architectures', 1), ('use of gaussian mixture models [', 1), ('use of recurrent neural networks', 1), ('used features', 1), ('v-a', 1), ('valence', 1), ('valence-arousal', 2), ('valence-arousal space', 1), ('values', 1), ('variety of moods', 1), ('various approaches', 1), ('various classi ﬁers', 1), ('various representations', 1), ('various word', 1), ('vector representations', 1), ('verse', 1), ('visual ones', 1), ('vocal part', 1), ('vocal tracks', 1), ('voice', 2), ('word', 2), ('word embedding', 1), ('word embeddings', 2), ('word embeddings with character information', 1), ('word2vec method', 1), ('years', 2), ('ﬁeld', 1), ('ﬁeld of mer', 1), ('ﬁeld of nlp', 1), ('ﬁeld of psychology', 1), ('ﬁnding', 1), ('ﬁrst studies', 1), ('ﬁrst sung line', 2), ('ﬁrst time', 1)]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gmP-RI3-mhzA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def score_keyphrases_by_textrank(text, n_keywords=0.05):\n",
        "    from itertools import takewhile, tee\n",
        "    import networkx, nltk\n",
        "    \n",
        "    # tokenize for all words, and extract *candidate* words\n",
        "    words = [word.lower()\n",
        "             for sent in nltk.sent_tokenize(text)\n",
        "             for word in nltk.word_tokenize(sent)]\n",
        "    candidates = extract_candidate_words(text)\n",
        "    # build graph, each node is a unique candidate\n",
        "    graph = networkx.Graph()\n",
        "    graph.add_nodes_from(set(candidates))\n",
        "    # iterate over word-pairs, add unweighted edges into graph\n",
        "    def pairwise(iterable):\n",
        "        \"\"\"s -> (s0,s1), (s1,s2), (s2, s3), ...\"\"\"\n",
        "        a, b = tee(iterable)\n",
        "        next(b, None)\n",
        "        return zip(a, b)\n",
        "    for w1, w2 in pairwise(candidates):\n",
        "        if w2:\n",
        "            graph.add_edge(*sorted([w1, w2]))\n",
        "    # score nodes using default pagerank algorithm, sort by score, keep top n_keywords\n",
        "    ranks = networkx.pagerank(graph)\n",
        "    if 0 < n_keywords < 1:\n",
        "        n_keywords = int(round(len(candidates) * n_keywords))\n",
        "    word_ranks = {word_rank[0]: word_rank[1]\n",
        "                  for word_rank in sorted(ranks.items(), key=lambda x: x[1], reverse=True)[:n_keywords]}\n",
        "    keywords = set(word_ranks.keys())\n",
        "    # merge keywords into keyphrases\n",
        "    keyphrases = {}\n",
        "    j = 0\n",
        "    for i, word in enumerate(words):\n",
        "        if i < j:\n",
        "            continue\n",
        "        if word in keywords:\n",
        "            kp_words = list(takewhile(lambda x: x in keywords, words[i:i+10]))\n",
        "            avg_pagerank = sum(word_ranks[w] for w in kp_words) / float(len(kp_words))\n",
        "            keyphrases[' '.join(kp_words)] = avg_pagerank\n",
        "            # counter as hackish way to ensure merged keyphrases are non-overlapping\n",
        "            j = i + len(kp_words)\n",
        "    \n",
        "    return sorted(keyphrases.items(), key=lambda x: x[1], reverse=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QPJJje_3CBUB",
        "colab_type": "code",
        "outputId": "926b3bec-e96e-4cbd-fbc7-54e620cdaed8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "score_keyphrases_by_textrank(text)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('audio', 0.020772356010959545),\n",
              " ('audio features', 0.015940980040515616),\n",
              " ('music', 0.015056242947485416),\n",
              " ('lyrics', 0.014441885556177585),\n",
              " ('music emotion', 0.014074422805978502),\n",
              " ('emotion', 0.013092602664471589),\n",
              " ('music features', 0.01308292350877855),\n",
              " ('audio data', 0.012708279325535445),\n",
              " ('music information', 0.01175624019437015),\n",
              " ('emotions', 0.011338641233589263),\n",
              " ('features', 0.011109604070071685),\n",
              " ('music emotion classi ﬁcation', 0.010623139495342838),\n",
              " ('musical lyrics', 0.010621753673476389),\n",
              " ('music emotion recognition', 0.010592043378812343),\n",
              " ('song', 0.009883447995928064),\n",
              " ('text', 0.009710032537633986),\n",
              " ('synchronized lyrics', 0.00964845681597988),\n",
              " ('synchronized lyrics emotion dataset', 0.009275090585631198),\n",
              " ('music emotion classi ﬁcation mer', 0.009242094020785055),\n",
              " ('emotion classi ﬁcation', 0.009145438344628646),\n",
              " ('text information', 0.009083134989444434),\n",
              " ('musical emotions', 0.009070131512182228),\n",
              " ('emotion dataset', 0.008901724355282515),\n",
              " ('emotion representation', 0.008741907677303684),\n",
              " ('different emotions', 0.008687189642713752),\n",
              " ('information', 0.008456237441254882),\n",
              " ('emotion recognition', 0.008359943594475805),\n",
              " ('mood', 0.008334082811579432),\n",
              " ('models', 0.008290833681524265),\n",
              " ('text representations', 0.008231011245282967),\n",
              " ('musical emotion representation', 0.00809514571512752),\n",
              " ('various emotions', 0.008018867364109183),\n",
              " ('text classi ﬁcation', 0.00801791496901611),\n",
              " ('synchronized lyrics dataset', 0.008002586559351066),\n",
              " ('many features', 0.007953491640576116),\n",
              " ('word', 0.007905236535202245),\n",
              " ('different text', 0.007872885294736114),\n",
              " ('musical emotions many', 0.007645880745148335),\n",
              " ('task models', 0.007571747589937982),\n",
              " ('musical mood', 0.007567852301177312),\n",
              " ('spectral features', 0.0075370513009411985),\n",
              " ('approach', 0.007449409831926214),\n",
              " ('results', 0.007373425694160379),\n",
              " ('word representations', 0.007328613244067096),\n",
              " ('text data', 0.007177117588872667),\n",
              " ('classi ﬁcation', 0.007171856184707173),\n",
              " ('classi', 0.007031898052060591),\n",
              " ('task', 0.0068526614983517005),\n",
              " ('musical', 0.006801621790775193),\n",
              " ('representations', 0.006751989952931947),\n",
              " ('time information', 0.0064752350455415456),\n",
              " ('approaches', 0.006308924093589212),\n",
              " ('various word', 0.006302165014915675),\n",
              " ('different', 0.006035738051838241),\n",
              " ('various classi', 0.0058654957733448474),\n",
              " ('different techniques', 0.005785486618943818),\n",
              " ('deep neural models', 0.0057463031427939315),\n",
              " ('various representations', 0.005725541723780526),\n",
              " ('many different approaches', 0.005714013785502667),\n",
              " ('nlp task', 0.005586066595861507),\n",
              " ('mer approach', 0.005583660977240069),\n",
              " ('techniques', 0.005535235186049396),\n",
              " ('various approaches', 0.005504008794109158),\n",
              " ('convolutional approaches', 0.005399748800644526),\n",
              " ('spectral representations', 0.0053582442423713295),\n",
              " ('different moods', 0.005324985895008602),\n",
              " ('different representation', 0.005213475370987009),\n",
              " ('large', 0.0049180159061453475),\n",
              " ('synchronized', 0.0048550280757821765),\n",
              " ('many', 0.004797379211080548),\n",
              " ('deep', 0.004782225246027186),\n",
              " ('dataset', 0.00471084604609344),\n",
              " ('segments', 0.0047082929532480285),\n",
              " ('data', 0.004644202640111348),\n",
              " ('moods', 0.004614233738178963),\n",
              " ('time', 0.0044942326498282085),\n",
              " ('convolutional', 0.004490573507699839),\n",
              " ('words', 0.004476192461719403),\n",
              " ('representation', 0.004391212690135778),\n",
              " ('language', 0.004386644579700734),\n",
              " ('convolutional neural', 0.0043282120042650914),\n",
              " ('nlp', 0.004319471693371313),\n",
              " ('synchronized combination', 0.004315628717508277),\n",
              " ('order', 0.004217024281885762),\n",
              " ('figure', 0.00417204907575461),\n",
              " ('neural', 0.004165850500830344),\n",
              " ('lstm', 0.00411307981091002),\n",
              " ('part', 0.0040892282060933614),\n",
              " ('years', 0.0039763054163384735),\n",
              " ('space', 0.003971438263718507),\n",
              " ('spectral', 0.003964498531810711),\n",
              " ('combination', 0.003776229359234378),\n",
              " ('mer', 0.003717912122553923),\n",
              " ('ﬁeld', 0.0036673603644603956),\n",
              " ('recognition', 0.003627284524480021)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 181
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_r-OLHwdmlS2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def extract_candidate_features(candidates, doc_text, doc_excerpt, doc_title):\n",
        "    import collections, math, nltk, re\n",
        "    \n",
        "    candidate_scores = collections.OrderedDict()\n",
        "    \n",
        "    # get word counts for document\n",
        "    doc_word_counts = collections.Counter(word.lower()\n",
        "                                          for sent in nltk.sent_tokenize(doc_text)\n",
        "                                          for word in nltk.word_tokenize(sent))\n",
        "    \n",
        "    for candidate in candidates:\n",
        "        \n",
        "        pattern = re.compile(r'\\b'+re.escape(candidate)+r'(\\b|[,;.!?]|\\s)', re.IGNORECASE)\n",
        "        \n",
        "        # frequency-based\n",
        "        # number of times candidate appears in document\n",
        "        cand_doc_count = len(pattern.findall(doc_text))\n",
        "        # count could be 0 for multiple reasons; shit happens in a simplified example\n",
        "        if not cand_doc_count:\n",
        "            print('**WARNING:', candidate, 'not found!')\n",
        "            continue\n",
        "    \n",
        "        # statistical\n",
        "        candidate_words = candidate.split()\n",
        "        max_word_length = max(len(w) for w in candidate_words)\n",
        "        term_length = len(candidate_words)\n",
        "        # get frequencies for term and constituent words\n",
        "        sum_doc_word_counts = float(sum(doc_word_counts[w] for w in candidate_words))\n",
        "        try:\n",
        "            # lexical cohesion doesn't make sense for 1-word terms\n",
        "            if term_length == 1:\n",
        "                lexical_cohesion = 0.0\n",
        "            else:\n",
        "                lexical_cohesion = term_length * (1 + math.log(cand_doc_count, 10)) * cand_doc_count / sum_doc_word_counts\n",
        "        except (ValueError, ZeroDivisionError) as e:\n",
        "            lexical_cohesion = 0.0\n",
        "        \n",
        "        # positional\n",
        "        # found in title, key excerpt\n",
        "        in_title = 1 if pattern.search(doc_title) else 0\n",
        "        in_excerpt = 1 if pattern.search(doc_excerpt) else 0\n",
        "        # first/last position, difference between them (spread)\n",
        "        doc_text_length = float(len(doc_text))\n",
        "        first_match = pattern.search(doc_text)\n",
        "        abs_first_occurrence = first_match.start() / doc_text_length\n",
        "        if cand_doc_count == 1:\n",
        "            spread = 0.0\n",
        "            abs_last_occurrence = abs_first_occurrence\n",
        "        else:\n",
        "            for last_match in pattern.finditer(doc_text):\n",
        "                pass\n",
        "            abs_last_occurrence = last_match.start() / doc_text_length\n",
        "            spread = abs_last_occurrence - abs_first_occurrence\n",
        "\n",
        "        candidate_scores[candidate] = {'term_count': cand_doc_count,\n",
        "                                       'term_length': term_length, 'max_word_length': max_word_length,\n",
        "                                       'spread': spread, 'lexical_cohesion': lexical_cohesion,\n",
        "                                       'in_excerpt': in_excerpt, 'in_title': in_title,\n",
        "                                       'abs_first_occurrence': abs_first_occurrence,\n",
        "                                       'abs_last_occurrence': abs_last_occurrence}\n",
        "\n",
        "    return candidate_scores"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Knyw6SHwU6B0",
        "colab_type": "code",
        "outputId": "cf8a1868-ffde-4774-aa3b-bf9db73d4d23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import json\n",
        "\n",
        "candidates = extract_candidate_words(text)\n",
        "candidates = candidates[0:5]\n",
        "candidate_features = extract_candidate_features(candidates, text, abstract, title)\n",
        "candidate_features = [{k[0]:k[1]} for k in sorted(candidate_features.items(), key=lambda item: item[1]['term_count'], reverse=True)]\n",
        "print(json.dumps(candidate_features,indent=4))"
      ],
      "execution_count": 191,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"lyrics\": {\n",
            "            \"term_count\": 26,\n",
            "            \"term_length\": 1,\n",
            "            \"max_word_length\": 6,\n",
            "            \"spread\": 0.9258158185840708,\n",
            "            \"lexical_cohesion\": 0.0,\n",
            "            \"in_excerpt\": 1,\n",
            "            \"in_title\": 1,\n",
            "            \"abs_first_occurrence\": 0.00165929203539823,\n",
            "            \"abs_last_occurrence\": 0.9274751106194691\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"features\": {\n",
            "            \"term_count\": 12,\n",
            "            \"term_length\": 1,\n",
            "            \"max_word_length\": 8,\n",
            "            \"spread\": 0.9496681415929203,\n",
            "            \"lexical_cohesion\": 0.0,\n",
            "            \"in_excerpt\": 1,\n",
            "            \"in_title\": 1,\n",
            "            \"abs_first_occurrence\": 0.00283462389380531,\n",
            "            \"abs_last_occurrence\": 0.9525027654867256\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"synchronized\": {\n",
            "            \"term_count\": 11,\n",
            "            \"term_length\": 1,\n",
            "            \"max_word_length\": 12,\n",
            "            \"spread\": 0.8889657079646018,\n",
            "            \"lexical_cohesion\": 0.0,\n",
            "            \"in_excerpt\": 1,\n",
            "            \"in_title\": 1,\n",
            "            \"abs_first_occurrence\": 0.0007605088495575221,\n",
            "            \"abs_last_occurrence\": 0.8897262168141593\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"vocal\": {\n",
            "            \"term_count\": 3,\n",
            "            \"term_length\": 1,\n",
            "            \"max_word_length\": 5,\n",
            "            \"spread\": 0.3024059734513274,\n",
            "            \"lexical_cohesion\": 0.0,\n",
            "            \"in_excerpt\": 0,\n",
            "            \"in_title\": 1,\n",
            "            \"abs_first_occurrence\": 0.002419800884955752,\n",
            "            \"abs_last_occurrence\": 0.30482577433628316\n",
            "        }\n",
            "    },\n",
            "    {\n",
            "        \"exploiting\": {\n",
            "            \"term_count\": 1,\n",
            "            \"term_length\": 1,\n",
            "            \"max_word_length\": 10,\n",
            "            \"spread\": 0.0,\n",
            "            \"lexical_cohesion\": 0.0,\n",
            "            \"in_excerpt\": 0,\n",
            "            \"in_title\": 1,\n",
            "            \"abs_first_occurrence\": 0.0,\n",
            "            \"abs_last_occurrence\": 0.0\n",
            "        }\n",
            "    }\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hBQ6aD0tj84a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "0b411036-e8b3-4220-80b5-9b183c761d95"
      },
      "source": [
        "candidates = [{word:score} for word,score in score_keyphrases_by_textrank(text)]\n",
        "candidates = candidates[0:5]\n",
        "print(json.dumps(candidates,indent=4))"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[\n",
            "    {\n",
            "        \"audio\": 0.020772356010959545\n",
            "    },\n",
            "    {\n",
            "        \"audio features\": 0.015940980040515616\n",
            "    },\n",
            "    {\n",
            "        \"music\": 0.015056242947485416\n",
            "    },\n",
            "    {\n",
            "        \"lyrics\": 0.014441885556177585\n",
            "    },\n",
            "    {\n",
            "        \"music emotion\": 0.014074422805978502\n",
            "    }\n",
            "]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}